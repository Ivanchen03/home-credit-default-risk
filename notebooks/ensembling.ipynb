{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model ensembling with weighted average\n",
    "\n",
    "This is a much simplified model ensembling as it only includes three single models.\n",
    "In the full solution, with 27 single models and 5 open solution models, ensembling can achieve a boost of ~0.002.\n",
    "\n",
    "private LB: 0.8004, public LB: 0.8046, local CV: 0.8028"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read single model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import gc\n",
    "\n",
    "# ======================================================================================\n",
    "# 步骤 1: 设置文件路径并加载数据\n",
    "# ======================================================================================\n",
    "print(\"Step 1: Loading and preparing data...\")\n",
    "\n",
    "# 定义输入文件路径\n",
    "train_files = ['/Users/chenzeyu/Documents/GitHub/home-credit-default-risk/output/train_pred_lgb1.csv',\n",
    "               '/Users/chenzeyu/Documents/GitHub/home-credit-default-risk/output/train_pred_lgb2.csv',\n",
    "               '/Users/chenzeyu/Documents/GitHub/home-credit-default-risk/output/train_pred_lgb3.csv']\n",
    "test_files = ['../output/test_pred_lgb1.csv', \n",
    "              '../output/test_pred_lgb2.csv', \n",
    "              '../output/test_pred_lgb3.csv']\n",
    "\n",
    "n_models = len(train_files)\n",
    "\n",
    "# --- 加载训练数据 ---\n",
    "# 读取第一个文件作为基础\n",
    "train_df = pd.read_csv(train_files[0])\n",
    "# 重命名预测列，以便区分\n",
    "train_df.rename(columns={'prob': 'lgb1_prob'}, inplace=True)\n",
    "\n",
    "# 循环读取并合并后续文件，确保 SK_ID_CURR 对齐\n",
    "for i in range(1, n_models):\n",
    "    model_name = f\"lgb{i+1}\"\n",
    "    preds_df = pd.read_csv(train_files[i])\n",
    "    preds_df.rename(columns={'prob': f'{model_name}_prob'}, inplace=True)\n",
    "    train_df = pd.merge(train_df, preds_df[['SK_ID_CURR', f'{model_name}_prob']], on='SK_ID_CURR', how='left')\n",
    "\n",
    "# 提取特征 (X) 和目标 (y)\n",
    "feature_cols = [f'lgb{i+1}_prob' for i in range(n_models)]\n",
    "train_x = train_df[feature_cols]\n",
    "train_y = train_df['target']\n",
    "\n",
    "# --- 加载测试数据 ---\n",
    "# 读取第一个文件作为基础\n",
    "test_df = pd.read_csv(test_files[0])\n",
    "# 注意：Kaggle 提交文件中的预测列通常名为 'TARGET'\n",
    "test_df.rename(columns={'TARGET': 'lgb1_prob'}, inplace=True)\n",
    "\n",
    "# 循环读取并合并后续文件\n",
    "for i in range(1, n_models):\n",
    "    model_name = f\"lgb{i+1}\"\n",
    "    preds_df = pd.read_csv(test_files[i])\n",
    "    preds_df.rename(columns={'TARGET': f'{model_name}_prob'}, inplace=True)\n",
    "    test_df = pd.merge(test_df, preds_df[['SK_ID_CURR', f'{model_name}_prob']], on='SK_ID_CURR', how='left')\n",
    "\n",
    "# 提取测试集的特征和 ID\n",
    "test_x = test_df[feature_cols]\n",
    "test_id = test_df['SK_ID_CURR']\n",
    "\n",
    "print(f\"Training features shape: {train_x.shape}\")\n",
    "print(f\"Test features shape: {test_x.shape}\")\n",
    "\n",
    "\n",
    "# ======================================================================================\n",
    "# 步骤 2: 诊断单个模型的性能 (与您原脚本功能类似)\n",
    "# ======================================================================================\n",
    "print(\"\\nStep 2: Evaluating individual model performance...\")\n",
    "for col in train_x.columns:\n",
    "    auc = roc_auc_score(train_y, train_x[col])\n",
    "    print(f\"Model '{col}' single AUC: {auc:.6f}\")\n",
    "\n",
    "\n",
    "# ======================================================================================\n",
    "# 步骤 3: 使用线性模型进行 Stacking (核心改动)\n",
    "# ======================================================================================\n",
    "print(\"\\nStep 3: Stacking models using Logistic Regression with Cross-Validation...\")\n",
    "\n",
    "# 初始化用于存储 Out-of-Fold 预测和测试集预测的数组\n",
    "oof_preds = np.zeros(train_x.shape[0])\n",
    "test_preds = np.zeros(test_x.shape[0])\n",
    "\n",
    "# 使用分层 K 折交叉验证来训练元模型，更稳健\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for n_fold, (trn_idx, val_idx) in enumerate(kf.split(train_x, train_y)):\n",
    "    print(f\"--- Training Fold {n_fold + 1} ---\")\n",
    "    \n",
    "    # 划分训练集和验证集\n",
    "    trn_x_fold, val_x_fold = train_x.iloc[trn_idx], train_x.iloc[val_idx]\n",
    "    trn_y_fold, val_y_fold = train_y.iloc[trn_idx], train_y.iloc[val_idx]\n",
    "    \n",
    "    # 定义线性模型 (元模型)\n",
    "    # C=0.1 提供了轻微的正则化，防止过拟合\n",
    "    meta_model = LogisticRegression(C=0.1, random_state=42)\n",
    "    \n",
    "    # 在训练折上训练元模型\n",
    "    meta_model.fit(trn_x_fold, trn_y_fold)\n",
    "    \n",
    "    # 在验证折上进行预测，并存储到 OOF 数组中\n",
    "    oof_preds[val_idx] = meta_model.predict_proba(val_x_fold)[:, 1]\n",
    "    \n",
    "    # 在整个测试集上进行预测，并累加（后续会求平均）\n",
    "    test_preds += meta_model.predict_proba(test_x)[:, 1] / kf.get_n_splits()\n",
    "\n",
    "# 计算总的 Out-of-Fold AUC 分数\n",
    "oof_auc = roc_auc_score(train_y, oof_preds)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Stacking Model OOF AUC: {oof_auc:.6f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "\n",
    "# ======================================================================================\n",
    "# 步骤 4: 生成提交文件\n",
    "# ======================================================================================\n",
    "print(\"\\nStep 4: Generating submission file...\")\n",
    "\n",
    "submission = pd.DataFrame({'SK_ID_CURR': test_id, 'TARGET': test_preds})\n",
    "submission.to_csv('submission_stacking_linear.csv', index=False)\n",
    "\n",
    "print(\"Submission file 'submission_stacking_linear.csv' created successfully.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check correlation between single model predictions, idealy we want low correlation for larger diversition between single models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation train:\n",
      "          lgb1      lgb2      lgb3\n",
      "lgb1  1.000000  0.990643  0.981924\n",
      "lgb2  0.990643  1.000000  0.984013\n",
      "lgb3  0.981924  0.984013  1.000000\n",
      "correlation test:\n",
      "          lgb1      lgb2      lgb3\n",
      "lgb1  1.000000  0.993755  0.990406\n",
      "lgb2  0.993755  1.000000  0.992537\n",
      "lgb3  0.990406  0.992537  1.000000\n"
     ]
    }
   ],
   "source": [
    "print('correlation train:')\n",
    "print(train_x.corr())\n",
    "print('correlation test:')\n",
    "print(test_x.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blending and check blended model local CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     lgb1, auc 0.802002   weight: 0.3333\n",
      "                     lgb2, auc 0.801779   weight: 0.3333\n",
      "                     lgb3, auc 0.801157   weight: 0.3333\n",
      "stacking model auc: train 0.802830\n"
     ]
    }
   ],
   "source": [
    "weights = [1.0/3, 1.0/3, 1.0/3]\n",
    "train_pred = pd.Series(np.zeros([train_x.shape[0]]))\n",
    "test_pred = pd.Series(np.zeros([test_x.shape[0]]))\n",
    "\n",
    "for i in range(n_model):\n",
    "    train_pred += weights[i] * train_x.iloc[:,i].values\n",
    "    test_pred += weights[i] * test_x.iloc[:,i].values\n",
    "    print ('%25s, auc %.6f   weight: %.4f' %(train_x.columns.values[i], roc_auc_score(train_y,train_x.iloc[:,i]), weights[i]))\n",
    "\n",
    "print ('stacking model auc: train %.6f' %(roc_auc_score(train_y,train_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save test prediction to disk. This will be our final submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['SK_ID_CURR'] = test_id\n",
    "sub['TARGET'] = test_pred\n",
    "sub.to_csv('../output/stacked_sub.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
